{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c22a870",
   "metadata": {},
   "source": [
    "Known protein modeler (ProtBERT, ESMfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e33f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fe124e7",
   "metadata": {},
   "source": [
    "Use known LLM to determine binding pockets etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094032d4",
   "metadata": {},
   "source": [
    "After parsing the input data should look like this, with a fasta protein ID, the protein sequence, and the 0/1 binary determining binding pocket residues\n",
    "\n",
    ">protein_id\n",
    "SEQUENCE\n",
    "000000000011111000000000000000000000000000000000000000000000000000000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4997c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.61 s\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from Bio import SeqIO\n",
    "import gzip\n",
    "\n",
    "# Load protein sequences\n",
    "def load_fasta_sequences(fasta_path):\n",
    "    sequences = {}\n",
    "    with gzip.open(fasta_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            sequences[record.id] = str(record.seq)\n",
    "    return sequences\n",
    "\n",
    "# Parse BioLiP.txt\n",
    "def parse_biolip_annotations(biolip_path):\n",
    "    binding_map = {}  # protein_id -> set of binding residue indices\n",
    "    with gzip.open(biolip_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            pdb_id = parts[0]\n",
    "            chain_id = parts[1]\n",
    "            binding_residues = parts[8]  # e.g., \"A:45,A:67,A:89\"\n",
    "            key = f\"{pdb_id}_{chain_id}\"\n",
    "            indices = set()\n",
    "            for res in binding_residues.split(\",\"):\n",
    "                if \":\" in res:\n",
    "                    _, idx = res.split(\":\")\n",
    "                    if idx.isdigit():\n",
    "                        indices.add(int(idx))\n",
    "            binding_map[key] = indices\n",
    "    return binding_map\n",
    "\n",
    "# Generate labeled output\n",
    "def generate_labeled_sequences(sequences, binding_map, output_path):\n",
    "    with open(output_path, \"w\") as out:\n",
    "        for protein_id, seq in sequences.items():\n",
    "            if protein_id not in binding_map:\n",
    "                continue\n",
    "            labels = [\"0\"] * len(seq)\n",
    "            for idx in binding_map[protein_id]:\n",
    "                if 0 <= idx < len(seq):\n",
    "                    labels[idx] = \"1\"\n",
    "            out.write(f\">{protein_id}\\n{seq}\\n{''.join(labels)}\\n\")\n",
    "\n",
    "# Paths to your files\n",
    "fasta_path = \"protein.fasta.gz\"\n",
    "biolip_path = \"BioLiP.txt.gz\"\n",
    "output_path = \"biolip_labeled.txt\"\n",
    "\n",
    "# Run preprocessing\n",
    "sequences = load_fasta_sequences(fasta_path)\n",
    "binding_map = parse_biolip_annotations(biolip_path)\n",
    "generate_labeled_sequences(sequences, binding_map, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f71bc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 32\n",
      "C: 34\n",
      "D: 35\n",
      "E: 36\n",
      "F: 37\n",
      "G: 38\n",
      "H: 39\n",
      "I: 40\n",
      "K: 42\n",
      "L: 43\n",
      "M: 44\n",
      "N: 45\n",
      "P: 47\n",
      "Q: 48\n",
      "R: 49\n",
      "S: 50\n",
      "T: 51\n",
      "V: 53\n",
      "W: 54\n",
      "Y: 56\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 531 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Add amino acid tokens (if needed)\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "tokenizer.add_tokens(amino_acids)\n",
    "\n",
    "for aa in amino_acids:\n",
    "    print(f\"{aa}: {tokenizer.convert_tokens_to_ids(aa)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40835b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 256)\n",
      "    (wpe): Embedding(1024, 256)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x GPT2Block(\n",
      "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=768, nx=256)\n",
      "          (c_proj): Conv1D(nf=256, nx=256)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=1024, nx=256)\n",
      "          (c_proj): Conv1D(nf=256, nx=1024)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
      ")\n",
      "Total parameters: 17,867,008\n",
      "CPU times: total: 5.31 s\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer), n_embd=256, n_layer=6, n_head=4)\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.nn as nn\n",
    "\n",
    "class PocketPredictor(nn.Module):\n",
    "    def __init__(self, gpt_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        hidden = self.gpt.transformer(input_ids).last_hidden_state  # Extract the tensor\n",
    "        return self.head(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec2ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 83])\n",
      "Decoded sequence: S K M S D V K C T S V V L L S V L Q Q L R V E S S S K L W A Q C V Q L H N D I L L A K D T T E A F E K M V S L L S V L L S M Q G A V D I N K L C E E M L D N R A T L Q\n",
      "Token IDs: [50, 42, 44, 50, 35, 53, 42, 34, 51, 50, 53, 53, 43, 43, 50, 53, 43, 48, 48, 43, 49, 53, 36, 50, 50, 50, 42, 43, 54, 32, 48, 34, 53, 48, 43, 39, 45, 35, 40, 43, 43, 32, 42, 35, 51, 51, 36, 32, 37, 36, 42, 44, 53, 50, 43, 43, 50, 53, 43, 43, 50, 44, 48, 38, 32, 53, 35, 40, 45, 42, 43, 34, 36, 36, 44, 43, 35, 45, 49, 32, 51, 43, 48]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"SKMSDVKCTSVVLLSVLQQLRVESSSKLWAQCVQLHNDILLAKDTTEAFEKMVSLLSVLLSMQGAVDINKLCEEMLDNRATLQ\"\n",
    "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")  # shape: [1, seq_len]\n",
    "\n",
    "print(\"Input shape:\", input_ids.shape)\n",
    "\n",
    "decoded = tokenizer.decode(input_ids[0])\n",
    "print(\"Decoded sequence:\", decoded)\n",
    "print(\"Token IDs:\", input_ids[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1e69291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 406 ms\n",
      "Wall time: 19 ms\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not BaseModelOutputWithPastAndCrossAttentions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport torch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mpocket_model = PocketPredictor(model, hidden_dim=256)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mwith torch.no_grad():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    output = pocket_model(input_ids)  # shape: [1, 83, 1]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    y_pred_probs = output.squeeze(0).squeeze(-1).numpy()  # shape: [83]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1452\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1416\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1414\u001b[39m st = clock2()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:6\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:17\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_ids)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear(): argument 'input' (position 1) must be Tensor, not BaseModelOutputWithPastAndCrossAttentions"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "\n",
    "pocket_model = PocketPredictor(model, hidden_dim=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pocket_model(input_ids)  # shape: [1, 83, 1]\n",
    "    y_pred_probs = output.squeeze(0).squeeze(-1).numpy()  # shape: [83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# y_true: binary labels (0/1), y_pred_probs: predicted probabilities\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_probs > threshold).astype(int)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pocket = set(i for i, label in enumerate(y_true) if label == 1)\n",
    "predicted_pocket = set(i for i, prob in enumerate(y_pred_probs) if prob > threshold)\n",
    "\n",
    "coverage = len(true_pocket & predicted_pocket) / len(true_pocket)\n",
    "overlap = len(true_pocket & predicted_pocket) / len(predicted_pocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe90eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch 1XYZ, async=0\n",
    "select predicted_pocket, resi 45+67+89  # replace with predicted residue indices\n",
    "color red, predicted_pocket\n",
    "show surface, predicted_pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5af3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_pocket_predictions(y_true, y_pred_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate binding pocket predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): Binary ground truth labels (0 = non-pocket, 1 = pocket)\n",
    "        y_pred_probs (np.array): Predicted probabilities for each residue\n",
    "        threshold (float): Classification threshold for binary decision\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_probs > threshold).astype(int)\n",
    "\n",
    "    # Residue-level metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Pocket-level coverage\n",
    "    true_pocket = set(np.where(y_true == 1)[0])\n",
    "    predicted_pocket = set(np.where(y_pred == 1)[0])\n",
    "    coverage = len(true_pocket & predicted_pocket) / len(true_pocket) if true_pocket else 0.0\n",
    "    overlap = len(true_pocket & predicted_pocket) / len(predicted_pocket) if predicted_pocket else 0.0\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"Pocket Coverage\": coverage,\n",
    "        \"Pocket Overlap\": overlap\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67632f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated predictions\n",
    "y_true = np.array([0, 0, 1, 1, 0, 0, 1, 0, 0])\n",
    "y_pred_probs = np.array([0.1, 0.2, 0.8, 0.7, 0.3, 0.2, 0.9, 0.1, 0.05])\n",
    "\n",
    "metrics = evaluate_pocket_predictions(y_true, y_pred_probs)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "view = nv.show_file(\"1XYZ.pdb\")\n",
    "view.add_representation(\"spacefill\", selection=\"45 or 67 or 89\", color=\"red\")\n",
    "view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
