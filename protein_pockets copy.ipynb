{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f33bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:20:11) [MSC v.1938 64 bit (AMD64)]\n",
      "Torch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA compiled: 12.4\n",
      "cuDNN version: 90100\n",
      "GPU count: 1\n",
      "Device 0: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA compiled:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}:\", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"No CUDA-visible device in this environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effcf09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences parsed from FASTA: 475001\n",
      "Total chains with binding annotations: 474607\n",
      "Number of overlapping IDs: 474607\n",
      "Example overlapping IDs: ['1bi3_b', '4tvw_d', '4m19_a', '6jjk_c', '1zjb_b', '8fc5_2v', '4qij_e', '4rdq_b', '6zqe_dn', '6ndk_yw']\n",
      "Dataset size: 474607\n",
      "First entry sequence length: 154\n",
      "First entry labels sum (pocket residues): 8\n",
      "CPU times: total: 35.4 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Canonical ID helper: use consistently for sequences & binding_map\n",
    "def canonical_id(pdb_id, chain_id):\n",
    "    \"\"\"\n",
    "    Normalize a PDB+chain ID to a consistent form, e.g. '1abc_A' -> '1abc_a'.\n",
    "    \"\"\"\n",
    "    return f\"{pdb_id.lower()}_{chain_id.lower()}\"\n",
    "\n",
    "# Load protein sequences\n",
    "def load_fasta_sequences(fasta_path):\n",
    "    sequences = {}\n",
    "    with gzip.open(fasta_path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            raw_id = record.id.split()[0]  # remove any trailing description\n",
    "\n",
    "            # ---- ID PARSING: this may need adjusting depending on your FASTA headers ----\n",
    "            # First, handle simple \"1abcA\" style IDs\n",
    "            if \"_\" not in raw_id and len(raw_id) >= 5:\n",
    "                pdb_id = raw_id[:4]\n",
    "                chain_id = raw_id[4:]\n",
    "            # If there *is* an underscore, assume \"1abc_A\" style and use first 4 chars as pdb,\n",
    "            # first char after \"_\" as chain, ignore trailing stuff\n",
    "            elif \"_\" in raw_id:\n",
    "                left, right = raw_id.split(\"_\", 1)\n",
    "                pdb_id = left[:4]\n",
    "                chain_id = right[0] if len(right) > 0 else \"A\"\n",
    "            else:\n",
    "                # Anything weird, skip\n",
    "                continue\n",
    "\n",
    "            key = canonical_id(pdb_id, chain_id)\n",
    "            sequences[key] = str(record.seq)\n",
    "    return sequences\n",
    "\n",
    "# Parse BioLiP.txt\n",
    "def parse_biolip_annotations(biolip_path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        binding_map: dict[canonical_id] -> set(0-based residue indices)\n",
    "\n",
    "    Uses column 9 from BioLiP (binding residues renumbered starting from 1),\n",
    "    as described in the official readme.txt.\n",
    "    \"\"\"\n",
    "    binding_map = {}\n",
    "\n",
    "    with gzip.open(biolip_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # BioLiP is tab-delimited\n",
    "            parts = line.split(\"\\t\")\n",
    "            # According to the readme, there are 21 columns; we just need the first 9+\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "\n",
    "            pdb_id = parts[0]          # column 1\n",
    "            chain_id = parts[1]        # column 2\n",
    "            key = canonical_id(pdb_id, chain_id)\n",
    "\n",
    "            # Column 9 (index 8): binding residues, renumbered starting from 1\n",
    "            # Example: \"N73 L74 A75 V108 H111 ...\"\n",
    "            binding_field = parts[8]\n",
    "\n",
    "            indices = []\n",
    "            for token in binding_field.split():\n",
    "                # Tokens are usually like \"N73\" or \"A182\". Extract the numeric part.\n",
    "                digits = \"\".join(ch for ch in token if ch.isdigit())\n",
    "                if not digits:\n",
    "                    continue\n",
    "                idx = int(digits) - 1   # convert from 1-based → 0-based\n",
    "                if idx >= 0:\n",
    "                    indices.append(idx)\n",
    "\n",
    "            if not indices:\n",
    "                continue\n",
    "\n",
    "            # Merge indices if the same chain appears multiple times\n",
    "            binding_map.setdefault(key, set()).update(indices)\n",
    "\n",
    "    return binding_map\n",
    "\n",
    "# Generate labeled output\n",
    "def generate_labeled_sequences(sequences, binding_map, output_path):\n",
    "    with open(output_path, \"w\") as out:\n",
    "        for protein_id, seq in sequences.items():\n",
    "            if protein_id not in binding_map:\n",
    "                continue\n",
    "            labels = [\"0\"] * len(seq)\n",
    "            for idx in binding_map[protein_id]:\n",
    "                if 0 <= idx < len(seq):\n",
    "                    labels[idx] = \"1\"\n",
    "            out.write(f\">{protein_id}\\n{seq}\\n{''.join(labels)}\\n\")\n",
    "\n",
    "# Paths to your files\n",
    "fasta_path = \"protein.fasta.gz\"\n",
    "biolip_path = \"BioLiP.txt.gz\"\n",
    "output_path = \"biolip_labeled.txt\"\n",
    "\n",
    "# Run preprocessing\n",
    "sequences = load_fasta_sequences(fasta_path)\n",
    "binding_map = parse_biolip_annotations(biolip_path)\n",
    "generate_labeled_sequences(sequences, binding_map, output_path)\n",
    "\n",
    "def load_labeled_dataset(labeled_path):\n",
    "    \"\"\"\n",
    "    Read the labeled BioLiP file and return a list of (sequence, labels).\n",
    "    Assumes the file is in repeating 3-line blocks:\n",
    "      >protein_id\n",
    "      SEQUENCE\n",
    "      001010...\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(labeled_path) as f:\n",
    "        # Remove empty lines to avoid misalignment\n",
    "        raw_lines = f.read().splitlines()\n",
    "        lines = [ln for ln in raw_lines if ln.strip()]\n",
    "\n",
    "    if len(lines) % 3 != 0:\n",
    "        print(f\"Warning: labeled file has {len(lines)} non-empty lines, \"\n",
    "              f\"which is not a multiple of 3. Truncating the last incomplete record.\")\n",
    "\n",
    "    # Go only up to len(lines)-2 so i+1 and i+2 are always valid\n",
    "    for i in range(0, len(lines) - 2, 3):\n",
    "        header = lines[i]\n",
    "        if not header.startswith(\">\"):\n",
    "            raise ValueError(f\"Expected header starting with '>' at line {i}, got: {header!r}\")\n",
    "\n",
    "        protein_id = header[1:]   # remove \">\"\n",
    "        seq = lines[i + 1]\n",
    "        labels_line = lines[i + 2]\n",
    "\n",
    "        labels = np.array([int(x) for x in labels_line])\n",
    "        # Sanity check: labels and sequence must match in length\n",
    "        if len(labels) != len(seq):\n",
    "            raise ValueError(\n",
    "                f\"Length mismatch for {protein_id}: seq={len(seq)}, labels={len(labels)}\"\n",
    "            )\n",
    "\n",
    "        dataset.append((seq, labels))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Now actually set it\n",
    "output_path = \"biolip_labeled.txt\"\n",
    "dataset = load_labeled_dataset(output_path)\n",
    "\n",
    "# --- Debug stats so we can see what's going on ---\n",
    "print(\"Total sequences parsed from FASTA:\", len(sequences))\n",
    "print(\"Total chains with binding annotations:\", len(binding_map))\n",
    "common_ids = set(sequences.keys()) & set(binding_map.keys())\n",
    "print(\"Number of overlapping IDs:\", len(common_ids))\n",
    "print(\"Example overlapping IDs:\", list(common_ids)[:10])\n",
    "\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "if len(dataset) > 0:\n",
    "    print(\"First entry sequence length:\", len(dataset[0][0]))\n",
    "    print(\"First entry labels sum (pocket residues):\", np.sum(dataset[0][1]))\n",
    "else:\n",
    "    print(\"No labeled sequences were written to biolip_labeled.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0828a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 691])\n",
      "Decoded sequence matches original: True\n",
      "First 30 token IDs: [14, 14, 14, 14, 15, 17, 13, 18, 1, 0, 17, 15, 13, 12, 3, 0, 16, 8, 1, 4, 13, 18, 13, 14, 11, 10, 14, 8, 17, 14]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Amino acid vocabulary (20 standard AAs)\n",
    "AA_VOCAB = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(AA_VOCAB)}\n",
    "\n",
    "class PocketPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Embedding layer for amino acids\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Simple feedforward layers\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch, seq_len]\n",
    "        x = self.embedding(input_ids)           # [batch, seq_len, embed_dim]\n",
    "        x = self.fc1(x)                         # [batch, seq_len, hidden_dim]\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)                         # [batch, seq_len, 1]\n",
    "        return self.sigmoid(x).squeeze(-1)      # [batch, seq_len]\n",
    "\n",
    "# Your query sequence (the one you want to predict pockets on)\n",
    "sequence = \"RRRRSVQWCAVSQPEATKCFQWQRNMRKVRGPPVSCIKRDSPIQCIQAIAENRADAVTLDGGFIYEAGLAPYKLRPVAAEVYGTERQPRTHYYAVAVVKKGGSFQLNELQGLKSCHTGLRRTAGWNVPIGTLRPFLNWTGPPEPIEAAVARFFSASCVPGADKGQFPNLCRLCAGTGENKCAFSSQEPYFSYSGAFKCLRDGAGDVAFIRESTVFEDLSDEAERDEYELLCPDNTRKPVDKFKDCHLARVPSHAVVARSVNGKEDAIWNLLRQAQEKFGKDKSPKFQLFGSPSGQKDLLFKDSAIGFSRVPPRIDSGLYLGSGYFTAIQNLRKSEEEVAARRARVVWCAVGEQELRKCNQWSGLSEGSVTCSSASTTEDCIALVLKGEADAMSLDEGYVYTAGKCGLVPVLAENYKSQQSSDPDPNCVDRPVEGYLAVAVVRRSDTSLTWNSVKGKKSCHTAVDRTAGWNIPMGLLFNQTGSCKFDEYFSQSCAPGSDPRSNLCALCIGDEQGENKCVPNSNERYYGYTGAFRCLAENAGDVAFVKDVTVLQNTDGNNNEAWAKDLKLADFALLCLDGKRKPVTEARSCHLAMAPNHAVVSRMDKVERLKQVLLHQQAKFGRNGSDCPDKFCLFQSETKNLLFNDNTECLARLHGKTTYEKYLGPQYVAGITNLKKCSTSPLLEACEFLRK\"\n",
    "\n",
    "# Check that the sequence only contains amino acids in our vocab\n",
    "invalid_aas = sorted({aa for aa in sequence if aa not in AA_VOCAB})\n",
    "if invalid_aas:\n",
    "    raise ValueError(f\"Sequence contains unknown AAs not in AA_VOCAB: {invalid_aas}\")\n",
    "\n",
    "# Encode to indices\n",
    "input_ids = torch.tensor([[aa_to_idx[aa] for aa in sequence]], dtype=torch.long)\n",
    "\n",
    "print(\"Input shape:\", input_ids.shape)\n",
    "\n",
    "# Decode back just to sanity-check mapping\n",
    "decoded_seq = \"\".join(AA_VOCAB[idx] for idx in input_ids[0].tolist())\n",
    "print(\"Decoded sequence matches original:\", decoded_seq == sequence)\n",
    "print(\"First 30 token IDs:\", input_ids[0][:30].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c6dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 475001\n",
      "Survive binding filter (existence only): 474607\n",
      "Survive length filter: 263046\n",
      "Final candidates: 263046\n",
      "Top 10 candidates: [('1bka_a', 0.9956268221574344), ('7n88_b', 0.9956268221574344), ('1cb6_a', 0.9941944847605225), ('1fck_a', 0.9912917271407837), ('2pms_a', 0.9908814589665653), ('2pms_b', 0.9908814589665653), ('1lct_a', 0.9906832298136646), ('1h44_a', 0.990625), ('1b0l_a', 0.9898403483309144), ('2bjj_x', 0.9869375907111756)]\n",
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def normalized_kmer_score(seq1, seq2, k=3):\n",
    "    kmers1 = Counter(seq1[i:i+k] for i in range(len(seq1)-k+1))\n",
    "    kmers2 = Counter(seq2[i:i+k] for i in range(len(seq2)-k+1))\n",
    "    shared = sum((kmers1 & kmers2).values())\n",
    "    denom = min(sum(kmers1.values()), sum(kmers2.values()))\n",
    "    return shared / denom if denom > 0 else 0.0\n",
    "\n",
    "scores = []\n",
    "total = len(sequences)\n",
    "with_binding = 0\n",
    "with_length = 0\n",
    "\n",
    "for pid, target_seq in sequences.items():\n",
    "    # sequences and binding_map now share the same canonical IDs\n",
    "    if pid not in binding_map:\n",
    "        continue\n",
    "    with_binding += 1\n",
    "\n",
    "    ratio = len(target_seq) / len(sequence)\n",
    "    if ratio < 0.3 or ratio > 10.0:\n",
    "        continue\n",
    "    with_length += 1\n",
    "\n",
    "    score = normalized_kmer_score(sequence, target_seq, k=3)\n",
    "    scores.append((pid, score))\n",
    "\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "top_candidates = scores[:50]\n",
    "\n",
    "print(\"Total sequences:\", total)\n",
    "print(\"Survive binding filter (existence only):\", with_binding)\n",
    "print(\"Survive length filter:\", with_length)\n",
    "print(\"Final candidates:\", len(scores))\n",
    "print(\"Top 10 candidates:\", top_candidates[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1538c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xincr\\OneDrive\\Desktop\\protLLM-2\\.venv\\Lib\\site-packages\\Bio\\pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match after prefilter: 1cb6_a\n",
      "Local alignment score: 1377.0\n",
      "CPU times: total: 3.64 s\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "best_match = None\n",
    "best_score = -1\n",
    "\n",
    "for pid, _ in top_candidates:\n",
    "    target_seq = sequences[pid]\n",
    "    alignments = pairwise2.align.localms(sequence, target_seq,\n",
    "                                         2, -1, -0.5, -0.1,\n",
    "                                         one_alignment_only=True)\n",
    "    score = alignments[0].score\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_match = pid\n",
    "\n",
    "print(\"Best match after prefilter:\", best_match)\n",
    "print(\"Local alignment score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d093f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned query (first 120 chars):    RRRRSVQWCAVSQPEATKCFQWQRNMRKVRGPPVSCIKRDSPIQCIQAIAENRADAVTLDGGFIYEAGLAPYKLRPVAAEVYGTERQPRTHYYAVAVVKKGGSFQLNELQGLKSCHTGLRRTAGWNVPIGTLRPFLNWTGPPEPIEAAVARFFSASCVPGADKGQFPNLCRLCAGTGENKCAFSSQEPYFSYSGAFKCLRDGAGDVAFIRESTVFEDLSDEAERDEYELLCPDNTRKPVDKFKDCHLARVPSHAVVARSVNGKEDAIWNLLRQAQEKFGKDKSPKFQLFGSPSGQKDLLFKDSAIGFSRVPPRIDSGLYLGSGYFTAIQNLRKSEEEVAARRARVVWCAVGEQELRKCNQWSGLSEGSVTCSSASTTEDCIALVLKGEADAMSLDEGYVYTAGKCGLVPVLAENYKSQQSSDPDPNCVDRPVEGYLAVAVVRRSDTSLTWNSVKGKKSCHTAVDRTAGWNIPMGLLFNQTGSCKFDEYFSQSCAPGSDPRSNLCALCIGDEQGENKCVPNSNERYYGYTGAFRCLAENAGDVAFVKDVTVLQNTDGNNNEAWAKDLKLADFALLCLDGKRKPVTEARSCHLAMAPNHAVVSRMDKVERLKQVLLHQQAKFGRNGSDCPDKFCLFQSETKNLLFNDNTECLARLHGKTTYEKYLGPQYVAGITNLKKCSTSPLLEACEFLRK\n",
      "Aligned template (first 120 chars): GRRRSVQWCAVSQPEATKCFQWQRNMRKVRGPPVSCIKRDSPIQCIQAIAENRADAVTLDGGFIYEAGLAPYKLRPVAAEVYGTERQPRTHYYAVAVVKKGGSFQLNELQGLKSCHTGLRRTAGWNVPIGTLRPFLNWTGPPEPIEAAVARFFSASCVPGADKGQFPNLCRLCAGTGENKCAFSSQEPYFSYSGAFKCLRDGAGDVAFIRESTVFEDLSDEAERDEYELLCPDNTRKPVDKFKDCHLARVPSHAVVARSVNGKEDAIWNLLRQAQEKFGKDKSPKFQLFGSPSGQKDLLFKDSAIGFSRVPPRIDSGLYLGSGYFTAIQNLRKSEEEVAARRARVVWCAVGEQELRKCNQWSGLSEGSVTCSSASTTEDCIALVLKGEADAMSLDGGYVYTAGKCGLVPVLAENYKSQQSSDPDPNCVDRPVEGYLAVAVVRRSDTSLTWNSVKGKKSCHTAVDRTAGWNIPMGLLFNQTGSCKFDEYFSQSCAPGSDPRSNLCALCIGDEQGENKCVPNSNERYYGYTGAFRCLAENAGDVAFVKDVTVLQNTDGNNNEAWAKDLKLADFALLCLDGKRKPVTEARSCHLAMAPNHAVVSRMDKVERLKQVLLHQQAKFGRNGSDCPDKFCLFQSETKNLLFNDNTECLARLHGKTTYEKYLGPQYVAGITNLKKCSTSPLLEACEFLRK\n",
      "Global alignment score:  1376.0\n",
      "y_true_global length: 691 positives: 6\n"
     ]
    }
   ],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "import numpy as np\n",
    "\n",
    "def transfer_binding_labels(query_seq, template_seq, binding_indices):\n",
    "    \"\"\"\n",
    "    Global alignment-based transfer:\n",
    "      - query_seq: your sequence (string or Seq)\n",
    "      - template_seq: template sequence (string or Seq)\n",
    "      - binding_indices: set of 0-based residue indices on the template\n",
    "    \"\"\"\n",
    "    alignment = pairwise2.align.globalms(query_seq, template_seq,\n",
    "                                         2, -1, -5, -1,\n",
    "                                         one_alignment_only=True)[0]\n",
    "    aligned_query = alignment.seqA\n",
    "    aligned_template = alignment.seqB\n",
    "\n",
    "    y_true = []\n",
    "    template_pos = -1\n",
    "\n",
    "    for q_char, t_char in zip(aligned_query, aligned_template):\n",
    "        if t_char != \"-\":\n",
    "            template_pos += 1\n",
    "        if q_char == \"-\":\n",
    "            # gap in query, skip (no label)\n",
    "            continue\n",
    "        y_true.append(1 if template_pos in binding_indices else 0)\n",
    "\n",
    "    return np.array(y_true), alignment\n",
    "\n",
    "# Example usage with the best_match chosen above\n",
    "query_seq = Seq(sequence)\n",
    "template_seq = Seq(sequences[best_match])\n",
    "binding_indices = binding_map[best_match]   # 0-based indices for this canonical ID\n",
    "\n",
    "y_true_global, alignment = transfer_binding_labels(query_seq, template_seq, binding_indices)\n",
    "\n",
    "print(\"Aligned query (first 120 chars):   \", alignment.seqA)\n",
    "print(\"Aligned template (first 120 chars):\", alignment.seqB)\n",
    "print(\"Global alignment score: \", alignment.score)\n",
    "print(\"y_true_global length:\", len(y_true_global), \"positives:\", np.sum(y_true_global))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1158f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training on 50000 sequences out of 474607\n",
      "Epoch 1/2, examples used: 49999, avg loss: 0.3294\n",
      "Epoch 2/2, examples used: 49999, avg loss: 0.3287\n",
      "CPU times: total: 2min 2s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Make sure AA_VOCAB, aa_to_idx, PocketPredictor, dataset are already defined\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "pocket_model = PocketPredictor(\n",
    "    vocab_size=len(AA_VOCAB),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=256\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(pocket_model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# For speed: train on a subset first (you can increase this later)\n",
    "MAX_TRAIN_SEQS = 50000   # try 10_000 if you want it really quick\n",
    "NUM_EPOCHS = 2           # bump to 5–10 later if you like\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:MAX_TRAIN_SEQS]\n",
    "\n",
    "print(f\"Training on {len(train_indices)} sequences out of {len(dataset)}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pocket_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    used = 0\n",
    "\n",
    "    for idx in train_indices:\n",
    "        seq, labels_np = dataset[idx]\n",
    "\n",
    "        # Skip sequences that contain non-standard AAs\n",
    "        if any(aa not in aa_to_idx for aa in seq):\n",
    "            continue\n",
    "\n",
    "        # Encode sequence into indices 0–19\n",
    "        inputs = torch.tensor(\n",
    "            [aa_to_idx[aa] for aa in seq],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        ).unsqueeze(0)  # [1, L]\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            labels_np,\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        ).unsqueeze(0)  # [1, L]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pocket_model(inputs)  # [1, L]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        used += 1\n",
    "\n",
    "    avg_loss = epoch_loss / max(used, 1)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, examples used: {used}, avg loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0006ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained prediction shape: (691,)\n",
      "Sample trained predictions: [0.22895603 0.22895603 0.22895603 0.22895603 0.10172698 0.05561124\n",
      " 0.09778902 0.1491107  0.23421668 0.06217684]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 2.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Ensure your 'sequence' string and aa_to_idx are already defined\n",
    "\n",
    "# Sanity: skip if sequence has unknown AAs\n",
    "invalid_aas = sorted({aa for aa in sequence if aa not in aa_to_idx})\n",
    "if invalid_aas:\n",
    "    raise ValueError(f\"Query sequence contains unknown AAs: {invalid_aas}\")\n",
    "\n",
    "pocket_model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor(\n",
    "        [[aa_to_idx[aa] for aa in sequence]],\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "    y_pred_probs = pocket_model(input_ids).squeeze(0).cpu().numpy()\n",
    "\n",
    "print(\"Trained prediction shape:\", y_pred_probs.shape)\n",
    "print(\"Sample trained predictions:\", y_pred_probs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c25103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true length: 691 positives: 6\n",
      "Alignment score: 1376.0\n",
      "Alignment:\n",
      "Query:    RRRRSVQWCAVSQPEATKCFQWQRNMRKVRGPPVSCIKRDSPIQCIQAIAENRADAVTLDGGFIYEAGLAPYKLRPVAAEVYGTERQPRTHYYAVAVVKKGGSFQLNELQGLKSCHTGLRRTAGWNVPIGTLRPFLNWTGPPEPIEAAVARFFSASCVPGADKGQFPNLCRLCAGTGENKCAFSSQEPYFSYSGAFKCLRDGAGDVAFIRESTVFEDLSDEAERDEYELLCPDNTRKPVDKFKDCHLARVPSHAVVARSVNGKEDAIWNLLRQAQEKFGKDKSPKFQLFGSPSGQKDLLFKDSAIGFSRVPPRIDSGLYLGSGYFTAIQNLRKSEEEVAARRARVVWCAVGEQELRKCNQWSGLSEGSVTCSSASTTEDCIALVLKGEADAMSLDEGYVYTAGKCGLVPVLAENYKSQQSSDPDPNCVDRPVEGYLAVAVVRRSDTSLTWNSVKGKKSCHTAVDRTAGWNIPMGLLFNQTGSCKFDEYFSQSCAPGSDPRSNLCALCIGDEQGENKCVPNSNERYYGYTGAFRCLAENAGDVAFVKDVTVLQNTDGNNNEAWAKDLKLADFALLCLDGKRKPVTEARSCHLAMAPNHAVVSRMDKVERLKQVLLHQQAKFGRNGSDCPDKFCLFQSETKNLLFNDNTECLARLHGKTTYEKYLGPQYVAGITNLKKCSTSPLLEACEFLRK\n",
      "Template: GRRRSVQWCAVSQPEATKCFQWQRNMRKVRGPPVSCIKRDSPIQCIQAIAENRADAVTLDGGFIYEAGLAPYKLRPVAAEVYGTERQPRTHYYAVAVVKKGGSFQLNELQGLKSCHTGLRRTAGWNVPIGTLRPFLNWTGPPEPIEAAVARFFSASCVPGADKGQFPNLCRLCAGTGENKCAFSSQEPYFSYSGAFKCLRDGAGDVAFIRESTVFEDLSDEAERDEYELLCPDNTRKPVDKFKDCHLARVPSHAVVARSVNGKEDAIWNLLRQAQEKFGKDKSPKFQLFGSPSGQKDLLFKDSAIGFSRVPPRIDSGLYLGSGYFTAIQNLRKSEEEVAARRARVVWCAVGEQELRKCNQWSGLSEGSVTCSSASTTEDCIALVLKGEADAMSLDGGYVYTAGKCGLVPVLAENYKSQQSSDPDPNCVDRPVEGYLAVAVVRRSDTSLTWNSVKGKKSCHTAVDRTAGWNIPMGLLFNQTGSCKFDEYFSQSCAPGSDPRSNLCALCIGDEQGENKCVPNSNERYYGYTGAFRCLAENAGDVAFVKDVTVLQNTDGNNNEAWAKDLKLADFALLCLDGKRKPVTEARSCHLAMAPNHAVVSRMDKVERLKQVLLHQQAKFGRNGSDCPDKFCLFQSETKNLLFNDNTECLARLHGKTTYEKYLGPQYVAGITNLKKCSTSPLLEACEFLRK\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "import numpy as np\n",
    "\n",
    "def transfer_binding_labels_global(query_seq, template_seq, binding_indices):\n",
    "    \"\"\"\n",
    "    Global alignment-based label transfer.\n",
    "\n",
    "    binding_indices: set/list of 0-based residue indices in the *template* sequence.\n",
    "    Returns:\n",
    "        y_true: np.array of shape [len(query_seq)], 0/1 per residue.\n",
    "        alignment: Biopython alignment object.\n",
    "    \"\"\"\n",
    "    alignment = pairwise2.align.globalms(\n",
    "        query_seq, template_seq,\n",
    "        2, -1,    # match, mismatch\n",
    "        -5, -1,   # gap open, gap extend\n",
    "        one_alignment_only=True\n",
    "    )[0]\n",
    "\n",
    "    aligned_query = alignment.seqA\n",
    "    aligned_template = alignment.seqB\n",
    "\n",
    "    y_true = []\n",
    "    template_pos = -1\n",
    "\n",
    "    for q_char, t_char in zip(aligned_query, aligned_template):\n",
    "        if t_char != \"-\":\n",
    "            template_pos += 1\n",
    "        if q_char == \"-\":\n",
    "            # gap in query: skip (no residue)\n",
    "            continue\n",
    "        y_true.append(1 if template_pos in binding_indices else 0)\n",
    "\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "\n",
    "    # If alignment weirdness causes small off-by-one issues, be forgiving:\n",
    "    if len(y_true) < len(query_seq):\n",
    "        y_true = np.pad(y_true, (0, len(query_seq) - len(y_true)))\n",
    "    elif len(y_true) > len(query_seq):\n",
    "        y_true = y_true[:len(query_seq)]\n",
    "\n",
    "    return y_true, alignment\n",
    "\n",
    "# Use your best_match template\n",
    "query_seq = Seq(sequence)\n",
    "template_seq = Seq(sequences[best_match])\n",
    "binding_indices = binding_map[best_match]   # 0-based indices\n",
    "\n",
    "y_true, alignment = transfer_binding_labels_global(\n",
    "    query_seq, template_seq, binding_indices\n",
    ")\n",
    "\n",
    "print(\"y_true length:\", len(y_true), \"positives:\", np.sum(y_true))\n",
    "print(\"Alignment score:\", alignment.score)\n",
    "print(\"Alignment:\")\n",
    "print(\"Query:   \", alignment.seqA)\n",
    "print(\"Template:\", alignment.seqB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e821fec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.698\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1: 0.000\n",
      "Pocket Coverage: 0.000\n",
      "Pocket Overlap: 0.000\n",
      "True pocket residues: 6\n",
      "Predicted pocket residues: 0\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_pocket_predictions(y_true, y_pred_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate binding pocket predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): Binary ground truth labels (0 = non-pocket, 1 = pocket)\n",
    "        y_pred_probs (np.array): Predicted probabilities for each residue\n",
    "        threshold (float): Classification threshold for binary decision\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred_probs):\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch: y_true={len(y_true)}, y_pred_probs={len(y_pred_probs)}\"\n",
    "        )\n",
    "\n",
    "    y_pred = (y_pred_probs > threshold).astype(int)\n",
    "\n",
    "    # Residue-level metrics\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # Pocket-level coverage / overlap\n",
    "    true_pocket = set(np.where(y_true == 1)[0])\n",
    "    predicted_pocket = set(np.where(y_pred == 1)[0])\n",
    "    intersection = true_pocket & predicted_pocket\n",
    "\n",
    "    coverage = len(intersection) / len(true_pocket) if true_pocket else 0.0\n",
    "    overlap = len(intersection) / len(predicted_pocket) if predicted_pocket else 0.0\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"Pocket Coverage\": coverage,\n",
    "        \"Pocket Overlap\": overlap,\n",
    "        \"True pocket residues\": len(true_pocket),\n",
    "        \"Predicted pocket residues\": len(predicted_pocket),\n",
    "    }\n",
    "\n",
    "metrics = evaluate_pocket_predictions(y_true, y_pred_probs, threshold=0.5)\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a38f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min prob: 0.055070921778678894\n",
      "Max prob: 0.23421667516231537\n",
      "Mean prob: 0.10539519786834717\n",
      "Quantiles: [0.09119008 0.22895603 0.23192109 0.23421668]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Min prob:\", float(y_pred_probs.min()))\n",
    "print(\"Max prob:\", float(y_pred_probs.max()))\n",
    "print(\"Mean prob:\", float(y_pred_probs.mean()))\n",
    "print(\"Quantiles:\", np.quantile(y_pred_probs, [0.5, 0.9, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169b252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.10 -> P=0.016, R=0.667, F1=0.032, predicted+=244\n",
      "thr=0.20 -> P=0.023, R=0.333, F1=0.043, predicted+=86\n",
      "thr=0.30 -> P=0.000, R=0.000, F1=0.000, predicted+=0\n",
      "thr=0.40 -> P=0.000, R=0.000, F1=0.000, predicted+=0\n",
      "thr=0.50 -> P=0.000, R=0.000, F1=0.000, predicted+=0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def metrics_at_threshold(y_true, y_pred_probs, threshold):\n",
    "    y_pred = (y_pred_probs > threshold).astype(int)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    num_pred = int(y_pred.sum())\n",
    "    return precision, recall, f1, num_pred\n",
    "\n",
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    p, r, f, n = metrics_at_threshold(y_true, y_pred_probs, thr)\n",
    "    print(f\"thr={thr:.2f} -> P={p:.3f}, R={r:.3f}, F1={f:.3f}, predicted+={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "135fd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k strategy (k = 6):\n",
      "Precision: 0.000, Recall: 0.000, F1: 0.000\n",
      "Top-k indices: [ 44 458 574 588 426   8]\n",
      "True pocket indices: [120 122 191 460 464 527]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "k = int(y_true.sum())  # number of true pockets = 6\n",
    "topk_idx = np.argsort(-y_pred_probs)[:k]  # indices of top-k residues\n",
    "\n",
    "y_pred_topk = np.zeros_like(y_true)\n",
    "y_pred_topk[topk_idx] = 1\n",
    "\n",
    "p = precision_score(y_true, y_pred_topk, zero_division=0)\n",
    "r = recall_score(y_true, y_pred_topk, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred_topk, zero_division=0)\n",
    "\n",
    "print(f\"Top-k strategy (k = {k}):\")\n",
    "print(f\"Precision: {p:.3f}, Recall: {r:.3f}, F1: {f1:.3f}\")\n",
    "print(\"Top-k indices:\", topk_idx)\n",
    "print(\"True pocket indices:\", np.where(y_true == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34066319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PocketPredictorLogits(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # [batch, seq_len] -> [batch, seq_len, embed_dim]\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)      # [batch, seq_len, 1]\n",
    "        return x.squeeze(-1) # [batch, seq_len] logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d537c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Estimated positive fraction: 0.06433\n",
      "Using pos_weight: 14.543944744092196\n",
      "Training (logits model) on 50000 sequences out of 474607\n",
      "[logits] Epoch 1/3, examples used: 49998, avg loss: 1.5366\n",
      "[logits] Epoch 2/3, examples used: 49998, avg loss: 1.5351\n",
      "[logits] Epoch 3/3, examples used: 49998, avg loss: 1.5346\n",
      "CPU times: total: 3min 38s\n",
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Rough estimate of positive / negative ratio on a subset\n",
    "sample_for_ratio = 10000\n",
    "all_pos = 0\n",
    "all_total = 0\n",
    "for i in random.sample(range(len(dataset)), min(sample_for_ratio, len(dataset))):\n",
    "    _, labels_np = dataset[i]\n",
    "    all_pos += labels_np.sum()\n",
    "    all_total += len(labels_np)\n",
    "\n",
    "pos_frac = all_pos / all_total\n",
    "neg_frac = 1.0 - pos_frac\n",
    "print(f\"Estimated positive fraction: {pos_frac:.5f}\")\n",
    "\n",
    "# pos_weight > 1 means we penalize missing a positive more than missing a negative\n",
    "pos_weight = torch.tensor([neg_frac / max(pos_frac, 1e-6)], device=device)\n",
    "print(\"Using pos_weight:\", float(pos_weight))\n",
    "\n",
    "pocket_model2 = PocketPredictorLogits(\n",
    "    vocab_size=len(AA_VOCAB),\n",
    "    embed_dim=64,\n",
    "    hidden_dim=256\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(pocket_model2.parameters(), lr=1e-4)\n",
    "\n",
    "MAX_TRAIN_SEQS = 50000\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:MAX_TRAIN_SEQS]\n",
    "\n",
    "print(f\"Training (logits model) on {len(train_indices)} sequences out of {len(dataset)}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pocket_model2.train()\n",
    "    epoch_loss = 0.0\n",
    "    used = 0\n",
    "\n",
    "    for idx in train_indices:\n",
    "        seq, labels_np = dataset[idx]\n",
    "\n",
    "        if any(aa not in aa_to_idx for aa in seq):\n",
    "            continue\n",
    "\n",
    "        inputs = torch.tensor(\n",
    "            [aa_to_idx[aa] for aa in seq],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        labels = torch.tensor(\n",
    "            labels_np,\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = pocket_model2(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        used += 1\n",
    "\n",
    "    avg_loss = epoch_loss / max(used, 1)\n",
    "    print(f\"[logits] Epoch {epoch+1}/{NUM_EPOCHS}, examples used: {used}, avg loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52987a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New prediction shape: (691,)\n",
      "New prob stats -> min/mean/max: 0.47272753715515137 0.6249222755432129 0.8391643166542053\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Predictions on your query sequence using the logits model\n",
    "invalid_aas = sorted({aa for aa in sequence if aa not in aa_to_idx})\n",
    "if invalid_aas:\n",
    "    raise ValueError(f\"Query sequence contains unknown AAs: {invalid_aas}\")\n",
    "\n",
    "pocket_model2.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor(\n",
    "        [[aa_to_idx[aa] for aa in sequence]],\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "    logits = pocket_model2(input_ids)  # [1, L]\n",
    "    y_pred_probs2 = torch.sigmoid(logits).squeeze(0).cpu().numpy()\n",
    "\n",
    "print(\"New prediction shape:\", y_pred_probs2.shape)\n",
    "print(\"New prob stats -> min/mean/max:\",\n",
    "      float(y_pred_probs2.min()),\n",
    "      float(y_pred_probs2.mean()),\n",
    "      float(y_pred_probs2.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86acb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics with logits + pos_weight (threshold=0.2):\n",
      "ROC-AUC: 0.737\n",
      "Precision: 0.009\n",
      "Recall: 1.000\n",
      "F1: 0.017\n",
      "Pocket Coverage: 1.000\n",
      "Pocket Overlap: 0.009\n",
      "True pocket residues: 6\n",
      "Predicted pocket residues: 691\n"
     ]
    }
   ],
   "source": [
    "metrics2 = evaluate_pocket_predictions(y_true, y_pred_probs2, threshold=0.2)\n",
    "\n",
    "print(\"\\nMetrics with logits + pos_weight (threshold=0.2):\")\n",
    "for k, v in metrics2.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96522a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.10 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.20 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.30 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.40 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.50 -> P=0.011, R=1.000, F1=0.022, predicted+=543\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    p, r, f, n = metrics_at_threshold(y_true, y_pred_probs2, thr)\n",
    "    print(f\"thr={thr:.2f} -> P={p:.3f}, R={r:.3f}, F1={f:.3f}, predicted+={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2243730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c936538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"Rostlab/prot_bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "print(\"Hidden size:\", base_model.config.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a6660f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ProtBertPocket(nn.Module):\n",
    "    def __init__(self, base_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = base_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_output = outputs.last_hidden_state      # [B, T, H]\n",
    "        seq_output = self.dropout(seq_output)\n",
    "        logits = self.classifier(seq_output).squeeze(-1)  # [B, T]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e146f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pocket_protbert = ProtBertPocket(base_model).to(device)\n",
    "\n",
    "# (optional) freeze encoder for cheap training\n",
    "for p in pocket_protbert.bert.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in pocket_protbert.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, pocket_protbert.parameters()),\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "274f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBert dataset size (after length filter): 467723\n",
      "Train batches: 125\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 338 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define max token / residue lengths for ProtBert\n",
    "MAX_TOKENS = 1024\n",
    "MAX_RESIDUES = MAX_TOKENS - 2  # CLS + SEP take 2 slots\n",
    "\n",
    "class BioLiPPocketProtDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_residues=None):\n",
    "        \"\"\"\n",
    "        dataset: list of (seq, labels_np)\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_residues = max_residues if max_residues is not None else MAX_RESIDUES\n",
    "\n",
    "        self.data = []\n",
    "        for seq, labels in dataset:\n",
    "            if len(seq) != len(labels):\n",
    "                continue\n",
    "            if len(seq) <= self.max_residues:\n",
    "                self.data.append((seq, labels))\n",
    "\n",
    "        print(f\"ProtBert dataset size (after length filter): {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, labels = self.data[idx]\n",
    "\n",
    "        # ProtBert expects space-separated AAs\n",
    "        seq_spaced = \" \".join(list(seq))\n",
    "\n",
    "        enc = tokenizer(\n",
    "            seq_spaced,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_residues + 2,  # CLS + residues + SEP\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)            # [T]\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)  # [T]\n",
    "\n",
    "        T = input_ids.size(0)\n",
    "\n",
    "        labels_full = torch.zeros(T, dtype=torch.float32)\n",
    "        residue_mask = torch.zeros(T, dtype=torch.bool)\n",
    "\n",
    "        L = min(len(labels), T - 2)  # residues kept\n",
    "\n",
    "        labels_full[1:1+L] = torch.tensor(labels[:L], dtype=torch.float32)\n",
    "        residue_mask[1:1+L] = True\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_full,\n",
    "            \"residue_mask\": residue_mask,\n",
    "            \"seq_len\": L,\n",
    "        }\n",
    "\n",
    "# Build dataset  (assumes `dataset` and `tokenizer` already exist)\n",
    "prot_dataset = BioLiPPocketProtDataset(dataset, tokenizer, max_residues=MAX_RESIDUES)\n",
    "\n",
    "# Subsample for training\n",
    "train_subset = 1000   # keep it small to test GPU speed; bump later\n",
    "BATCH_SIZE = 8        # adjust if you hit OOM\n",
    "\n",
    "indices = list(range(len(prot_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:train_subset]\n",
    "\n",
    "class SubsetDataset(Dataset):\n",
    "    def __init__(self, base_ds, indices):\n",
    "        self.base_ds = base_ds\n",
    "        self.indices = indices\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_ds[self.indices[idx]]\n",
    "\n",
    "train_ds = SubsetDataset(prot_dataset, train_indices)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84fa0930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/125\n",
      "Batch 40/125\n",
      "Batch 60/125\n",
      "Batch 80/125\n",
      "Batch 100/125\n",
      "Batch 120/125\n",
      "Epoch 1/3, avg loss: 0.6473\n",
      "Batch 20/125\n",
      "Batch 40/125\n",
      "Batch 60/125\n",
      "Batch 80/125\n",
      "Batch 100/125\n",
      "Batch 120/125\n",
      "Epoch 2/3, avg loss: 0.5590\n",
      "Batch 20/125\n",
      "Batch 40/125\n",
      "Batch 60/125\n",
      "Batch 80/125\n",
      "Batch 100/125\n",
      "Batch 120/125\n",
      "Epoch 3/3, avg loss: 0.4952\n",
      "CPU times: total: 2min 48s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPOCHS = 3  # start small to sanity-check speed\n",
    "\n",
    "pocket_protbert.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader, start=1):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)            # [B, T]\n",
    "        residue_mask = batch[\"residue_mask\"].to(device)  # [B, T]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = pocket_protbert(input_ids, attention_mask)  # [B, T]\n",
    "\n",
    "        # Only compute loss on residue positions\n",
    "        logits_res = logits[residue_mask]\n",
    "        labels_res = labels[residue_mask]\n",
    "\n",
    "        loss = criterion(logits_res, labels_res)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Batch {i}/{len(train_loader)}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, avg loss: {epoch_loss / max(n_batches,1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "753781f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBert dataset size (after length filter): 467723\n",
      "Train batches: 1250\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 332 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prot_dataset = BioLiPPocketProtDataset(dataset, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 4  # ProtBert is big; adjust based on GPU memory\n",
    "train_subset = 5000  # number of sequences to actually use for quick fine-tuning\n",
    "\n",
    "indices = list(range(len(prot_dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[:train_subset]\n",
    "\n",
    "class SubsetDataset(Dataset):\n",
    "    def __init__(self, base_ds, indices):\n",
    "        self.base_ds = base_ds\n",
    "        self.indices = indices\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_ds[self.indices[idx]]\n",
    "\n",
    "train_ds = SubsetDataset(prot_dataset, train_indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Train batches:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e17051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtBertPocket(nn.Module):\n",
    "    def __init__(self, base_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = base_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_output = outputs.last_hidden_state      # [B, T, H]\n",
    "        seq_output = self.dropout(seq_output)\n",
    "        logits = self.classifier(seq_output).squeeze(-1)  # [B, T]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27d30522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pocket_protbert = ProtBertPocket(base_model).to(device)\n",
    "\n",
    "# Option 1: freeze the BERT encoder, train only the head\n",
    "for param in pocket_protbert.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# If you want to fine-tune the whole model, comment out the loop above.\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, pocket_protbert.parameters()),\n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dcf73e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBert per-residue probs shape: (691,)\n",
      "First 10 probs: [0.49345312 0.48102584 0.49102947 0.47661766 0.47565004 0.49132064\n",
      " 0.48090282 0.5065823  0.51055634 0.4988366 ]\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pocket_protbert.eval()\n",
    "\n",
    "seq_spaced = \" \".join(list(sequence))\n",
    "enc = tokenizer(\n",
    "    seq_spaced,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_RESIDUES + 2,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids = enc[\"input_ids\"].to(device)\n",
    "attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_q = pocket_protbert(input_ids, attention_mask)  # [1, T]\n",
    "    probs_q = torch.sigmoid(logits_q).squeeze(0).cpu().numpy()  # [T]\n",
    "\n",
    "# Extract only the residue positions (skip CLS at 0, SEP at L+1, and padding)\n",
    "Lq = min(len(sequence), MAX_RESIDUES)\n",
    "y_pred_probs_prot = probs_q[1:1+Lq]  # [Lq]\n",
    "\n",
    "print(\"ProtBert per-residue probs shape:\", y_pred_probs_prot.shape)\n",
    "print(\"First 10 probs:\", y_pred_probs_prot[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35d86ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check shapes: 691 691\n",
      "=== ProtBert pocket model metrics (threshold=0.5) ===\n",
      "ROC-AUC: 0.420\n",
      "Precision: 0.004\n",
      "Recall: 0.167\n",
      "F1: 0.008\n",
      "Pocket Coverage: 0.167\n",
      "Pocket Overlap: 0.004\n",
      "True pocket residues: 6\n",
      "Predicted pocket residues: 249\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 9.53 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Check shapes:\", len(y_true), len(y_pred_probs_prot))\n",
    "\n",
    "metrics_prot = evaluate_pocket_predictions(y_true, y_pred_probs_prot, threshold=0.5)\n",
    "\n",
    "print(\"=== ProtBert pocket model metrics (threshold=0.5) ===\")\n",
    "for k, v in metrics_prot.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e89774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBert thr=0.10 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "ProtBert thr=0.20 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "ProtBert thr=0.30 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "ProtBert thr=0.40 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "ProtBert thr=0.50 -> P=0.004, R=0.167, F1=0.008, predicted+=249\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    p, r, f, n = metrics_at_threshold(y_true, y_pred_probs_prot, thr)\n",
    "    print(f\"ProtBert thr={thr:.2f} -> P={p:.3f}, R={r:.3f}, F1={f:.3f}, predicted+={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b21fc962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtBert Top-k (k=6) -> P=0.000, R=0.000, F1=0.000\n",
      "Top-k indices: [409  60  55 390 395 663]\n",
      "True pocket indices: [120 122 191 460 464 527]\n"
     ]
    }
   ],
   "source": [
    "k = int(y_true.sum())\n",
    "topk_idx_prot = np.argsort(-y_pred_probs_prot)[:k]\n",
    "\n",
    "y_pred_topk_prot = np.zeros_like(y_true)\n",
    "y_pred_topk_prot[topk_idx_prot] = 1\n",
    "\n",
    "p_k = precision_score(y_true, y_pred_topk_prot, zero_division=0)\n",
    "r_k = recall_score(y_true, y_pred_topk_prot, zero_division=0)\n",
    "f1_k = f1_score(y_true, y_pred_topk_prot, zero_division=0)\n",
    "\n",
    "print(f\"ProtBert Top-k (k={k}) -> P={p_k:.3f}, R={r_k:.3f}, F1={f1_k:.3f}\")\n",
    "print(\"Top-k indices:\", topk_idx_prot)\n",
    "print(\"True pocket indices:\", np.where(y_true == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6220b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequence): 691\n",
      "len(y_true): 691\n",
      "len(y_pred_probs_prot): 691\n",
      "ProtBert ROC-AUC: 0.4204379562043795\n"
     ]
    }
   ],
   "source": [
    "print(\"len(sequence):\", len(sequence))\n",
    "print(\"len(y_true):\", len(y_true))\n",
    "print(\"len(y_pred_probs_prot):\", len(y_pred_probs_prot))\n",
    "\n",
    "metrics_prot = evaluate_pocket_predictions(y_true, y_pred_probs_prot, threshold=0.5)\n",
    "print(\"ProtBert ROC-AUC:\", metrics_prot[\"ROC-AUC\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfc2eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AUC comparison ===\n",
      "Simple model ROC-AUC: 0.7369829683698297\n",
      "ProtBert ROC-AUC:     0.4204379562043795\n",
      "\n",
      "=== Threshold sweep: Simple model ===\n",
      "thr=0.10 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.20 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.30 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.40 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.50 -> P=0.011, R=1.000, F1=0.022, predicted+=543\n",
      "\n",
      "=== Threshold sweep: ProtBert ===\n",
      "thr=0.10 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.20 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.30 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.40 -> P=0.009, R=1.000, F1=0.017, predicted+=691\n",
      "thr=0.50 -> P=0.004, R=0.167, F1=0.008, predicted+=249\n"
     ]
    }
   ],
   "source": [
    "# choose your simple model probs\n",
    "y_pred_probs_simple = y_pred_probs2  # or y_pred_probs\n",
    "\n",
    "print(\"=== AUC comparison ===\")\n",
    "m_simple = evaluate_pocket_predictions(y_true, y_pred_probs_simple, threshold=0.5)\n",
    "m_prot   = evaluate_pocket_predictions(y_true, y_pred_probs_prot,   threshold=0.5)\n",
    "print(\"Simple model ROC-AUC:\", m_simple[\"ROC-AUC\"])\n",
    "print(\"ProtBert ROC-AUC:    \", m_prot[\"ROC-AUC\"])\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "print(\"\\n=== Threshold sweep: Simple model ===\")\n",
    "for thr in thresholds:\n",
    "    p, r, f, n = metrics_at_threshold(y_true, y_pred_probs_simple, thr)\n",
    "    print(f\"thr={thr:.2f} -> P={p:.3f}, R={r:.3f}, F1={f:.3f}, predicted+={n}\")\n",
    "\n",
    "print(\"\\n=== Threshold sweep: ProtBert ===\")\n",
    "for thr in thresholds:\n",
    "    p, r, f, n = metrics_at_threshold(y_true, y_pred_probs_prot, thr)\n",
    "    print(f\"thr={thr:.2f} -> P={p:.3f}, R={r:.3f}, F1={f:.3f}, predicted+={n}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
